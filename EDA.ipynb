{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composer Classification EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mido import MidiFile\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from mido import KeySignatureError\n",
    "import logging\n",
    "\n",
    "# nn libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New method that will and should get more data\n",
    "Hopefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = 'midiclassics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the midi file features\n",
    "def extract_midi_features(file_path, max_sequence_length=300):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        midi_data = pretty_midi.PrettyMIDI(file_path)\n",
    "        tempo = midi_data.estimate_tempo()\n",
    "        key_signatures = [key.key_number for key in midi_data.key_signature_changes]\n",
    "        time_signatures = [(time.numerator, time.denominator) for time in midi_data.time_signature_changes]\n",
    "        instrument_types = [instr.program for instr in midi_data.instruments]\n",
    "        notes_histogram = midi_data.get_pitch_class_histogram()\n",
    "        notes = np.zeros((max_sequence_length, 128))\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                start = int(note.start * max_sequence_length / midi_data.get_end_time())\n",
    "                end = int(note.end * max_sequence_length / midi_data.get_end_time())\n",
    "                notes[start:end, note.pitch] = note.velocity / 127\n",
    "        end_time = time.time()\n",
    "        print(f\"Processed {file_path} in {end_time - start_time:.2f} seconds\")\n",
    "        return {\n",
    "            'tempo': tempo,\n",
    "            'key_signatures': key_signatures,\n",
    "            'time_signatures': time_signatures,\n",
    "            'instrument_types': instrument_types,\n",
    "            'notes_histogram': notes_histogram.tolist(),\n",
    "            'notes': notes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get MIDI data from the Chopin folder\n",
    "def get_chopin_midi_data(main_dir, max_sequence_length=300):\n",
    "    composers_data = {}\n",
    "    chopin_folder = 'Chopin'\n",
    "    folder_path = os.path.join(main_dir, chopin_folder)\n",
    "    data = []\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if file_path.endswith('.midi') or file_path.endswith('.mid') or file_path.endswith('.MID'):\n",
    "                features = extract_midi_features(file_path, max_sequence_length)\n",
    "                if features is not None:\n",
    "                    features['file'] = file\n",
    "                    data.append(features)\n",
    "        composers_data[chopin_folder] = pd.DataFrame(data)\n",
    "    else:\n",
    "        print(f\"The folder {chopin_folder} does not exist in the directory {main_dir}.\")\n",
    "    \n",
    "    return composers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data right here \n",
    "start_time = time.time()\n",
    "chopin_data = get_chopin_midi_data(main_dir)\n",
    "end_time = time.time()\n",
    "print(f\"Total time to process all files: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shape of the data  \n",
    "chopin_data['Chopin'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chopin_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing some of the data, it is a dictonary\n",
    "chopin_data['Chopin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat GPT explination of the columns\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### `Tempo`\n",
    "\n",
    "Description: This represents the estimated tempo (beats per minute) of the MIDI file. Tempo is a crucial aspect of a musical piece as it dictates the speed at which the music is played.\n",
    "\n",
    "___\n",
    "\n",
    "### `Key Signatures`\n",
    "\n",
    "Description: These are the musical keys in which sections of the MIDI file are written. A key signature indicates the set of notes that are generally used in the piece, which provides a sense of tonality.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### `Time Signatures`\n",
    "\n",
    "Description: This indicates the time signature changes in the MIDI file. A time signature defines the number of beats in each measure and the note value that represents one beat.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### `Instrument Types`\n",
    "\n",
    "Description: This represents the different types of instruments used in the MIDI file. Each instrument is identified by a program number according to the General MIDI specification.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### `Notes Histogram`\n",
    "\n",
    "Description: This is a histogram of the pitch classes (notes) used in the MIDI file. It provides a frequency distribution of each pitch class (C, C#, D, etc.) over the entire piece.\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "### `Notes`\n",
    "\n",
    "Description: This is a matrix representing the notes played in the MIDI file over time. Each row corresponds to a time slice, and each column corresponds to a MIDI pitch (from 0 to 127). The value indicates the velocity (intensity) of the note.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pritng first notes histogram val \n",
    "chopin_data['Chopin']['notes_histogram'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chopin_data['Chopin']['notes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chopin_data['Chopin']['instrument_types'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_data = chopin_data['Chopin']['tempo']\n",
    "tempo_data.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the min and max of the width of notes array\n",
    "notes_data = chopin_data['Chopin']['notes']\n",
    "notes_widths = [np.where(notes.any(axis=1))[0].max() - np.where(notes.any(axis=1))[0].min() for notes in notes_data]\n",
    "notes_widths = np.array(notes_widths)\n",
    "notes_widths.min(), notes_widths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running some base EDA to see how the data looks and what We should do with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempos = chopin_data['Chopin']['tempo'].dropna()\n",
    "plt.hist(tempos, bins=20, edgecolor='black')\n",
    "plt.title('Tempo Distribution')\n",
    "plt.xlabel('Tempo (BPM)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_signatures = chopin_data['Chopin']['key_signatures'].explode().dropna()\n",
    "plt.hist(key_signatures, bins=range(22), edgecolor='black')\n",
    "plt.title('Key Signature Distribution')\n",
    "plt.xlabel('Key Signature (MIDI Number)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_signatures = chopin_data['Chopin']['time_signatures'].explode().dropna()\n",
    "time_signatures = time_signatures.apply(lambda x: f\"{x[0]}/{x[1]}\")\n",
    "time_signatures.value_counts().plot(kind='bar')\n",
    "plt.title('Time Signature Distribution')\n",
    "plt.xlabel('Time Signature')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instrument_types = chopin_data['Chopin']['instrument_types'].explode().dropna()\n",
    "plt.hist(instrument_types, bins=range(129), edgecolor='black')\n",
    "plt.title('Instrument Types Distribution')\n",
    "plt.xlabel('Instrument Program Number')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_histogram = chopin_data['Chopin']['notes_histogram'].explode().dropna()\n",
    "avg_notes_histogram = np.mean(notes_histogram.tolist(), axis=0)\n",
    "plt.bar(range(12), avg_notes_histogram)\n",
    "plt.title('Average Pitch Class Histogram')\n",
    "plt.xlabel('Pitch Class')\n",
    "plt.ylabel('Normalized Frequency')\n",
    "plt.xticks(range(12), ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_piece_notes = chopin_data['Chopin']['notes'].iloc[0]\n",
    "\n",
    "plt.imshow(specific_piece_notes.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Notes Matrix')\n",
    "plt.xlabel('Time (normalized)')\n",
    "plt.ylabel('MIDI Pitch')\n",
    "plt.colorbar(label='Velocity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_piece_notes = chopin_data['Chopin']['notes'].iloc[3]\n",
    "\n",
    "plt.imshow(specific_piece_notes.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Notes Matrix')\n",
    "plt.xlabel('Time (normalized)')\n",
    "plt.ylabel('MIDI Pitch')\n",
    "plt.colorbar(label='Velocity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def pitch_class_histogram_over_time(midi_data, max_sequence_length=300):\n",
    "    # Initialize a dictionary to hold pitch class histograms for each time slice\n",
    "    histograms = defaultdict(lambda: np.zeros(12))\n",
    "\n",
    "    # Fill histograms for each time slice\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            start_time = int(note.start * max_sequence_length / midi_data.get_end_time())\n",
    "            pitch_class = note.pitch % 12\n",
    "            histograms[start_time][pitch_class] += note.velocity\n",
    "\n",
    "    # Convert to a matrix\n",
    "    histogram_matrix = np.zeros((max_sequence_length, 12))\n",
    "    for time_slice, histogram in histograms.items():\n",
    "        histogram_matrix[time_slice, :] = histogram\n",
    "\n",
    "    return histogram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_data = pretty_midi.PrettyMIDI('midiclassics\\Chopin\\(2542)Prelude opus.28, No.16 in B flat minor.mid')\n",
    "histogram_matrix = pitch_class_histogram_over_time(midi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(histogram_matrix.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.title('Pitch Class Histogram Over Time')\n",
    "plt.xlabel('Time (normalized)')\n",
    "plt.ylabel('Pitch Class')\n",
    "plt.colorbar(label='Velocity')\n",
    "plt.xticks(range(12), ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to extract chords\n",
    "def extract_chords(midi_data, time_step=0.5):\n",
    "    chords = []\n",
    "    for t in np.arange(0, midi_data.get_end_time(), time_step):\n",
    "        notes = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                if note.start <= t < note.end:\n",
    "                    notes.append(note.pitch)\n",
    "        chords.append(Counter(notes))\n",
    "    return chords\n",
    "\n",
    "chords = extract_chords(midi_data)\n",
    "chords = [sorted(chord.items()) for chord in chords]\n",
    "\n",
    "# Plot chord progression\n",
    "plt.figure(figsize=(12, 8))\n",
    "for t, chord in enumerate(chords):\n",
    "    for note, count in chord:\n",
    "        plt.plot([t, t+1], [note, note], color='black', linewidth=count)\n",
    "plt.title('Chord Progression')\n",
    "plt.xlabel('Time (steps)')\n",
    "plt.ylabel('MIDI Pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I think we should try feeding into our model...\n",
    "\n",
    "`Tempo` The overall tempo of the piece provides information about the speed at which the piece is played.\n",
    "\n",
    "`Key Signatures:` The key signatures used throughout the piece can give insight into the tonality and harmonic structure.\n",
    "\n",
    "`Time Signatures:` The time signatures indicate the rhythmic structure of the piece.\n",
    "\n",
    "`Instrument Types:` The types of instruments used can be characteristic of a composer's style.\n",
    "\n",
    "`Notes Histogram:` A histogram of the pitch classes (notes) used in the piece can provide information about the melodic and harmonic content.\n",
    "\n",
    "`Notes Matrix:` The detailed matrix representing which notes are played over time and their velocities.\n",
    "\n",
    "`Rhythmic Features:` Extract features such as note density, average note duration, and rhythmic patterns.\n",
    "\n",
    "`Melodic Intervals:` The distribution of melodic intervals (differences in pitch between consecutive notes) can be indicative of a composer's style.\n",
    "\n",
    "`Chord Progressions:` The sequence of chords used throughout the piece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
