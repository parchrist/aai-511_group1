{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Composer classification Model**\n",
    "___\n",
    "\n",
    "#### **Problem Statement**\n",
    "\n",
    "We are going to be working with the MIDI files of classical music composers. We will be using the MIDI files to classify the composer of the music. We will be using the MIDI files of the following composers: Bach Beethoven, chopin and Mozart. We will be building a CNN network to classify the composers, and we will also be making functions that will make the process of dealing with the midi files easier to work with. The objective is to get the classification model to be at approximately 90% accuracy, or better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librarys for the model and working with data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# deep learning libraries for the CNN classification model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# working with the midi files\n",
    "import pretty_midi\n",
    "import librosa.display\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the MIDI files\n",
    "main_dir = 'midiclassics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from MIDI files\n",
    "def extract_midi_features(file_path, max_sequence_length=300):\n",
    "    try:\n",
    "        midi_data = pretty_midi.PrettyMIDI(file_path)\n",
    "        tempo = midi_data.estimate_tempo()\n",
    "        key_signatures = [key.key_number for key in midi_data.key_signature_changes]\n",
    "        time_signatures = [(time.numerator, time.denominator) for time in midi_data.time_signature_changes]\n",
    "        instrument_types = [instr.program for instr in midi_data.instruments]\n",
    "        notes_histogram = midi_data.get_pitch_class_histogram()\n",
    "        notes = np.zeros((max_sequence_length, 128))\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                start = int(note.start * max_sequence_length / midi_data.get_end_time())\n",
    "                end = int(note.end * max_sequence_length / midi_data.get_end_time())\n",
    "                notes[start:end, note.pitch] = note.velocity / 127\n",
    "        return {\n",
    "            'tempo': tempo,\n",
    "            'key_signatures': key_signatures,\n",
    "            'time_signatures': time_signatures,\n",
    "            'instrument_types': instrument_types,\n",
    "            'notes_histogram': notes_histogram.tolist(),\n",
    "            'notes': notes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get MIDI data from the main directory\n",
    "def get_midi_data(main_dir, max_sequence_length=300):\n",
    "    composers_data = {}\n",
    "    \n",
    "    # Function to process a single composer's folder\n",
    "    def process_composer_folder(folder):\n",
    "        folder_path = os.path.join(main_dir, folder)\n",
    "        data = []\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if file_path.endswith('.midi') or file_path.endswith('.mid') or file_path.endswith('.MID'):\n",
    "                features = extract_midi_features(file_path, max_sequence_length)\n",
    "                if features is not None:\n",
    "                    features['file'] = file\n",
    "                    data.append(features)\n",
    "        return folder, pd.DataFrame(data)\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel processing\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_composer_folder, folder) for folder in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, folder))]\n",
    "        for future in futures:\n",
    "            folder, df = future.result()\n",
    "            composers_data[folder] = df\n",
    "    \n",
    "    return composers_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MIDI data\n",
    "all_composers_data = get_midi_data(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def prepare_data(df, composer_name, max_sequence_length=300):\n",
    "    df['composer'] = composer_name\n",
    "    scaler = StandardScaler()\n",
    "    df['tempo'] = scaler.fit_transform(df[['tempo']])\n",
    "    df['padded_notes'] = pad_sequences(df['notes'].tolist(), maxlen=max_sequence_length, padding='post', dtype='float32').tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and split dataset\n",
    "def prepare_features_and_labels(combined_data):\n",
    "    X = np.array(combined_data['padded_notes'].tolist())\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(combined_data['composer'])\n",
    "    y_categorical = to_categorical(y)\n",
    "    return X, y_categorical, label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital model\n",
    "def create_initial_model(input_shape, n_classes, initial_units=64):\n",
    "    model = Sequential([\n",
    "        LSTM(initial_units, input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.5),\n",
    "        LSTM(initial_units // 2, return_sequences=True),\n",
    "        Dropout(0.5),\n",
    "        LSTM(initial_units // 4, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom training loop\n",
    "def progressive_training(X_train, y_train, X_test, y_test, initial_units=64, max_layers=10, peak_layers=6):\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    n_classes = y_train.shape[1]\n",
    "    model = create_initial_model(input_shape, n_classes, initial_units)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    current_layer_count = 3  # Starting with 3 LSTM layers\n",
    "\n",
    "    for i in range(max_layers):\n",
    "        checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='max')\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test),\n",
    "                  callbacks=[checkpoint, early_stopping])\n",
    "        model.load_weights('best_model.h5')\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            if best_accuracy >= 0.90:\n",
    "                print(\"Reached 90% accuracy. Stopping training...\")\n",
    "                break\n",
    "\n",
    "        if current_layer_count < peak_layers:\n",
    "            model.pop()  # Remove softmax layer\n",
    "            model.add(LSTM(initial_units // (2 ** (current_layer_count - 2)), return_sequences=(current_layer_count != peak_layers)))\n",
    "            model.add(Dropout(0.5))\n",
    "            if current_layer_count == peak_layers:\n",
    "                model.add(Dense(n_classes, activation='softmax'))\n",
    "            current_layer_count += 1\n",
    "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        else:\n",
    "            print(\"Reached maximum layers. Stopping training...\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tehwh\\anaconda3\\envs\\ml\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to process midiclassics\\Beethoven\\Anhang 14-3.mid: Could not decode key with 3 flats and mode 255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load MIDI data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_composers_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_midi_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mget_midi_data\u001b[1;34m(main_dir, max_sequence_length)\u001b[0m\n\u001b[0;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.midi\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_midi_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m file\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mextract_midi_features\u001b[1;34m(file_path, max_sequence_length)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m note \u001b[38;5;129;01min\u001b[39;00m instrument\u001b[38;5;241m.\u001b[39mnotes:\n\u001b[0;32m     13\u001b[0m         start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(note\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m max_sequence_length \u001b[38;5;241m/\u001b[39m midi_data\u001b[38;5;241m.\u001b[39mget_end_time())\n\u001b[1;32m---> 14\u001b[0m         end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(note\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m*\u001b[39m max_sequence_length \u001b[38;5;241m/\u001b[39m \u001b[43mmidi_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_end_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m         notes[start:end, note\u001b[38;5;241m.\u001b[39mpitch] \u001b[38;5;241m=\u001b[39m note\u001b[38;5;241m.\u001b[39mvelocity \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m127\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempo\u001b[39m\u001b[38;5;124m'\u001b[39m: tempo,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_signatures\u001b[39m\u001b[38;5;124m'\u001b[39m: key_signatures,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m'\u001b[39m: notes\n\u001b[0;32m     23\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\tehwh\\anaconda3\\envs\\ml\\lib\\site-packages\\pretty_midi\\pretty_midi.py:456\u001b[0m, in \u001b[0;36mPrettyMIDI.get_end_time\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# Get end times from all instruments, and times of all meta-events\u001b[39;00m\n\u001b[0;32m    452\u001b[0m meta_events \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_signature_changes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_signature_changes,\n\u001b[0;32m    453\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlyrics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_events]\n\u001b[0;32m    454\u001b[0m times \u001b[38;5;241m=\u001b[39m ([i\u001b[38;5;241m.\u001b[39mget_end_time() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruments] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    455\u001b[0m          [e\u001b[38;5;241m.\u001b[39mtime \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m meta_events \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m m] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m--> 456\u001b[0m          \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tempo_changes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m    457\u001b[0m \u001b[38;5;66;03m# If there are no events, return 0\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(times) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tehwh\\anaconda3\\envs\\ml\\lib\\site-packages\\pretty_midi\\pretty_midi.py:432\u001b[0m, in \u001b[0;36mPrettyMIDI.get_tempo_changes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return arrays of tempo changes in quarter notes-per-minute and their\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03mtimes.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    428\u001b[0m \n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# Pre-allocate return arrays\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m tempo_change_times \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tick_scales\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m tempi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tick_scales))\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, (tick, tick_scale) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tick_scales):\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;66;03m# Convert tick of this tempo change to time in seconds\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load MIDI data\n",
    "all_composers_data = get_midi_data(main_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "prepared_dfs = []\n",
    "for composer_name, df in all_composers_data.items():\n",
    "    prepared_dfs.append(prepare_data(df, composer_name, 300))\n",
    "combined_data = pd.concat(prepared_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data \n",
    "X, y, classes = prepare_features_and_labels(combined_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "model = progressive_training(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
